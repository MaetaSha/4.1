1.huffman
import heapq
from collections import Counter

def calculate_frequency(my_text):
    my_text = my_text.upper().replace(' ', '')
    frequency = dict(Counter(my_text))
    return frequency

def build_heap(freq):
    heap = [[weight, [char, ""]] for char, weight in freq.items()]
    heapq.heapify(heap)
    return heap

def build_tree(heap):
    while len(heap) > 1:
        lo = heapq.heappop(heap)
        hi = heapq.heappop(heap)
        for pair in lo[1:]:
            pair[1] = '0' + pair[1]
        for pair in hi[1:]:
            pair[1] = '1' + pair[1]
        heapq.heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])
    return heap[0]

# Test the functions
freq = calculate_frequency("aaabbbbbccccccddddee")
heap = build_heap(freq)
tree = build_tree(heap)

# Sort by character and print
for pair in sorted(tree[1:], key=lambda x: x[0]):
    print(pair[0], '->', pair[1])


2.convolution
import numpy as np

def encode(msg, K, n):
    g, v = [], []
    
    # Input generator bits
    for i in range(n):
        sub_g = list(map(int, input(f'Enter bits for generator {i}: ').split()))
        if len(sub_g) != K:
            raise ValueError(f'You entered {len(sub_g)} bits.\nNeed to enter {K} bits')
        g.append(sub_g)
    
    # Calculate encoded vectors
    for i in range(n):
        res = list(np.poly1d(g[i]) * np.poly1d(msg))
        v.append(res)
    
    # Padding to make lengths uniform
    listMax = max(len(l) for l in v)
    for i in range(n):
        if len(v[i]) != listMax:
            tmp = [0] * (listMax - len(v[i]))
            v[i] = tmp + v[i]
    
    # Final encoding by summing modulo 2
    res = []
    for i in range(listMax):
        res += [v[j][i] % 2 for j in range(n)]
    
    return res

# Get inputs
message = list(map(int, input('Enter message: ').split()))
K = int(input('Constraints: '))
n = int(input('Number of output (generators): '))
print('Encoded Message:', encode(message, K, n))


3.Limpel ziv
message = 'AABABBBABAABABBBABBABB'
dictionary = {}
tmp, i, last = '', 1, 0
Flag = True

# Build the dictionary based on the message
for x in message:
    tmp += x
    Flag = False
    if tmp not in dictionary.keys():
        dictionary[tmp] = i
        tmp = ''
        i += 1
        Flag = True

# Capture the last part if it wasn't processed
if not Flag:
    last = dictionary[tmp]

# Generate the encoded message based on the dictionary
res = ['1']
for char, idx in list(dictionary.items())[1:]:
    tmp, s = '', ''
    for j, x in enumerate(char[:-1]):
        tmp += x
    if tmp in dictionary.keys():
        take = dictionary[tmp]
        s = str(take) + char[j + 1:]
    if len(char) == 1:
        s = char
    res.append(s)

# Append the last dictionary reference if there was a leftover
if last:
    res.append(str(last))

# Mapping dictionary for binary conversion
mark = {'A': 0, 'B': 1}

# Convert encoded message to binary representation
final_res = []
for x in res:
    tmp = ""
    for char in x:
        if char.isalpha():
            tmp += bin(mark[char])[2:]  # Convert 'A' or 'B' to binary
        else:
            tmp += bin(int(char))[2:]  # Convert numbers to binary
    final_res.append(tmp.zfill(4))  # Zero-pad to 4 bits for uniformity

# Output the results
print("Dictionary:", dictionary)
print("Encoded Message (Symbolic):", res)
print("Encoded Message (Binary):", final_res)


4.Hamming code
def calculate_parity_bits(data_bits):
    # Place data bits in positions 3, 5, 6, and 7
    d3, d5, d6, d7 = map(int, data_bits)
    
    # Calculate parity bits for even parity
    # P1 covers positions 1, 3, 5, 7
    p1 = (d3 + d5 + d7) % 2
    # P2 covers positions 2, 3, 6, 7
    p2 = (d3 + d6 + d7) % 2
    # P4 covers positions 4, 5, 6, 7
    p4 = (d5 + d6 + d7) % 2

    # Return the encoded message
    hamming_code = f"{p1}{p2}{d3}{p4}{d5}{d6}{d7}"
    return hamming_code

def detect_and_correct_error(received_data):
    # Convert the received data to a list of integers for easier manipulation
    received_bits = list(map(int, received_data))

    # Check parity for P1, P2, and P4
    p1_check = (received_bits[0] + received_bits[2] + received_bits[4] + received_bits[6]) % 2
    p2_check = (received_bits[1] + received_bits[2] + received_bits[5] + received_bits[6]) % 2
    p4_check = (received_bits[3] + received_bits[4] + received_bits[5] + received_bits[6]) % 2

    # Calculate the syndrome (error position)
    error_position = p1_check * 1 + p2_check * 2 + p4_check * 4

    if error_position == 0:
        return "No error detected", received_data  # No error
    else:
        # Correct the error at the error_position
        print(f"Error detected at position {error_position}. Correcting it.")
        received_bits[error_position - 1] = 1 - received_bits[error_position - 1]  # Flip the erroneous bit
        return "Error detected and corrected", received_bits

# Example usage
data_bits = "1011"  # Data bits to encode
print(f"Data: {data_bits}")

# Calculate the encoded message (Hamming code)
encoded_data = calculate_parity_bits(data_bits)
print(f"Encoded Data: {encoded_data}")

# Simulate a transmission with an error (let's say bit 6 has an error)
received_data_with_error = "0110111"  # This is the received data with a single-bit error
print(f"Received Data (with error): {received_data_with_error}")

# Detect and correct errors
status, corrected_data = detect_and_correct_error(received_data_with_error)
print(status)
print(f"Corrected Data: {''.join(map(str, corrected_data))}")


5.Noise mattrix
import math

# Given symmetric matrix
matrix = [[2 / 3, 1 / 3], [1 / 3, 2 / 3]]
print("Symmetric matrix is:")
for i in range(2):
    for j in range(2):
        print(f'{matrix[i][j]:.2f} ', end=' ')
    print()

# Calculate H(Y|X) using the formula: H(Y|X) = (1-p)log(1/(1-p)) + p*log(1/p)
Hp = (
    matrix[0][0] * math.log2(1.0 / matrix[0][0]) +
    matrix[0][1] * math.log2(1.0 / matrix[0][1])
)

print(f"Conditional entropy H(Y|X) is = {Hp:.3f} bits/message symbol")

# Now calculate channel capacity using the formula: C = 1 - H(Y|X)
C = 1 - Hp
print(f"Channel Capacity is = {C:.3f} bits/message symbol")


6.Optimality huffman
import heapq
import math
from collections import Counter

# Calculate frequency of each character in the text
def calculate_frequency(my_text):
    my_text = my_text.upper().replace(' ', '')
    frequency = dict(Counter(my_text))
    return frequency

# Build a heap for Huffman coding
def build_heap(freq):
    heap = [[weight, [char, ""]] for char, weight in freq.items()]
    heapq.heapify(heap)
    return heap

# Build the Huffman Tree
def build_tree(heap):
    while len(heap) > 1:
        lo = heapq.heappop(heap)
        hi = heapq.heappop(heap)
        for pair in lo[1:]:
            pair[1] = '0' + pair[1]
        for pair in hi[1:]:
            pair[1] = '1' + pair[1]
        heapq.heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])
    return heap[0]

# Compute the average length of the Huffman code
def compute_huffman_avg_length(freq, tree, length):
    huffman_avg_length = 0
    for pair in tree[1:]:
        huffman_avg_length += (len(pair[1]) * (freq[pair[0]] / length))
    return huffman_avg_length

# Calculate entropy of the message
def entropy(freq, length):
    H = 0
    P = [fre / length for fre in freq.values()]
    for x in P:
        H += -(x * math.log2(x))
    return H

# Input message and calculations
message = "aaabbbbbccccccddddee"
freq = calculate_frequency(message)
heap = build_heap(freq)
tree = build_tree(heap)

# Calculate Huffman average length and entropy
huffman_avg_length = compute_huffman_avg_length(freq, tree, len(message))
H = entropy(freq, len(message))

# Print results
print("Huffman Average Length: %.2f bits" % huffman_avg_length)
print("Entropy: %.2f bits" % H)
if huffman_avg_length >= H:
    print("Huffman code is optimal")
else:
    print("Code is not optimal")


7.Weighted graph
import math
from collections import defaultdict

# Initialize the graph
g = defaultdict(list)

# Example input data
xij = [[1, 1, 2], [1, 1], [1, 2, 1], [1, 1]]

# Function to build the graph
def makeGraph(li):
    for node in range(len(li)):
        for x in li[node]:
            g[node].append(x)

# Calculate entropy for a list of probabilities
def entropy(li):
    H = 0
    for x in li:
        if x > 0:
            H += -(x * math.log2(x))
    return H

# Build the graph
makeGraph(xij)

# Calculate wi, the sum of edges connected to each node
wi = [sum(g[node]) for node in range(len(g))]

# Summation(wi) = 2 * w
w = sum(wi) / 2

# Stationary distribution ui = (wi) / (2 * w)
ui = [weight / (2 * w) for weight in wi]

# Calculate entropy H((wi) / 2w)
H_wi_div_2w = entropy(ui)

# Calculate H(wij / 2w)
wij_div_2w_list = []
for i in range(len(g)):
    wij_div_2w_list += [weight / (2 * w) for weight in g[i]]

H_wij_div_2w = entropy(wij_div_2w_list)

# Finally, the entropy rate H(x) = H(wij / 2w) - H(wi / 2w)
H_x = H_wij_div_2w - H_wi_div_2w
print('Entropy Rate: %.2f' % H_x)


8.Conditional and Joint Entropy. 
import math

# Given joint probability matrix
matrix = [
    [1 / 8, 1 / 16, 1 / 32, 1 / 32],
    [1 / 16, 1 / 8, 1 / 32, 1 / 32],
    [1 / 16, 1 / 16, 1 / 16, 1 / 16],
    [1 / 4, 0, 0, 0]
]

# Marginal distribution of x
marginal_x = []
for i in range(len(matrix[0])):
    marginal_x.append(sum(matrix[j][i] for j in range(len(matrix))))

# Marginal distribution of y
marginal_y = []
for i in range(len(matrix)):
    marginal_y.append(sum(matrix[i][j] for j in range(len(matrix[0]))))

# Calculate entropy for a given distribution
def entropy(marginal_var):
    H = 0
    for x in marginal_var:
        if x > 0:  # Ignore zero probabilities to avoid math errors
            H += -(x * math.log2(x))
    return H

# Calculate H(x) and H(y)
H_x = entropy(marginal_x)
H_y = entropy(marginal_y)

# Calculate conditional entropy H(x|y)
H_xy = 0
for i in range(len(matrix)):
    # Calculate P(x|y)
    tmp = [(matrix[i][j] / marginal_y[i]) for j in range(len(matrix[0]))]
    H_xy += entropy(tmp) * marginal_y[i]

# Calculate conditional entropy H(y|x)
H_yx = 0
for i in range(len(matrix[0])):
    # Calculate P(y|x)
    tmp = [(matrix[j][i] / marginal_x[i]) for j in range(len(matrix))]
    H_yx += entropy(tmp) * marginal_x[i]

# Print results
print('Conditional Entropy H(x|y): ', H_xy)
print('Conditional Entropy H(y|x): ', H_yx)

# Joint entropy H(x,y)
H_of_xy = H_x + H_yx
print('Joint Entropy H(x,y): ', H_of_xy)

# Mutual Information I(x,y)
I_of_xy = H_x + H_y - H_of_xy
print('Mutual Information: ', I_of_xy)

